{"name":"Pml","tagline":"","body":"ML Assignment\r\n---\r\n\r\n\r\n\r\n### Load Data\r\n\r\n```r\r\ntestData <- read.csv(\"pml-testing.csv\", colClasses=\"character\")\r\ntrainData <- read.csv(\"pml-training.csv\", colClasses=\"character\")\r\n```\r\n\r\n\r\n```r\r\ndim(trainData)\r\n```\r\n\r\n```\r\n## [1] 19622   160\r\n```\r\n\r\n```r\r\ndim(testData)\r\n```\r\n\r\n```\r\n## [1]  20 160\r\n```\r\n\r\n### Explore data\r\nExplore the input dataset. Dataset contains of 160 columns.  \r\nFirst column is entry id, second is 'user_name' and final 160th one is actual Class of exercise.  \r\nMore details about the dataset can be found here http://groupware.les.inf.puc-rio.br/har  \r\nThese three columns have to be extracted as factor. Other columsn 3:159 are numeric values.\r\n\r\n```r\r\ntrainData[,3:159]<-apply(trainData[,3:159], 2, as.numeric)\r\ntrainData[,1]<-as.factor(trainData[,1])\r\ntrainData[,2]<-as.factor(trainData[,2])\r\ntrainData[,160]<-as.factor(trainData[,160])\r\n\r\ntestData[,3:159]<-apply(testData[,3:159], 2, as.numeric)\r\ntestData[,1]<-as.factor(testData[,1])\r\ntestData[,2]<-as.factor(testData[,2])\r\ntestData[,160]<-as.factor(testData[,160])\r\n```\r\n\r\nFirst few columns in the dataset tracks entry ID, timestamp, Date etc  \r\nThese do not help in deciding the activity. Hence, these are removed from the dataset.\r\n\r\n```r\r\nnames(trainData[,1:7])\r\n```\r\n\r\n```\r\n## [1] \"X\"                    \"user_name\"            \"raw_timestamp_part_1\"\r\n## [4] \"raw_timestamp_part_2\" \"cvtd_timestamp\"       \"new_window\"          \r\n## [7] \"num_window\"\r\n```\r\n\r\n```r\r\ntrainData <- trainData[,c(-1, -3:-7)]\r\ntestData <- testData[,c(-1, -3:-7)]\r\n```\r\n\r\n\r\n### Remove missing data\r\n\r\n```r\r\nunique(colSums(!is.na(trainData)))\r\n```\r\n\r\n```\r\n##  [1] 19622   396   374     0   397   406   328   326   395   329   401\r\n## [12]   404   402   405   322   321   323\r\n```\r\nMany of the columns in the dataset have a lot of missing values (NA)  \r\nAs a safety, we remove all columns which have more than half the data missing.\r\n\r\n```r\r\nselCol <- colSums(!is.na(trainData))> (nrow(trainData)/2)\r\ntrainData <- trainData[,selCol]\r\ntestData <- testData[,selCol]\r\ndim(trainData)\r\n```\r\n\r\n```\r\n## [1] 19622    54\r\n```\r\n\r\n```r\r\ndim(testData)\r\n```\r\n\r\n```\r\n## [1] 20 54\r\n```\r\n\r\n\r\n### Training and Testing\r\n\r\n```r\r\nset.seed(3433)\r\ninTrain = createDataPartition(trainData$classe, p = 0.7, list=FALSE)\r\ntraining = trainData[inTrain,]\r\ntesting = trainData[-inTrain,]\r\n```\r\nAfter removing the unwanted columns we get a dataset with 54 columnns\r\n\r\n### Preprocess\r\n**Check for low variance factors**  \r\nIt is quite possible that there might be some features which have very low variance  \r\nWe want to remove these since it will slow down the training process without adding any accuracy.  \r\n'nearZeroVar' function checks if the descriptors are close to zero or have very low variance.\r\n\r\n```r\r\nnzv <- nearZeroVar(training, saveMetrics= TRUE)\r\nsum(nzv$nzv)\r\n```\r\n\r\n```\r\n## [1] 0\r\n```\r\nIn this case, there were no descriptors with very low variance.  \r\nHence no descriptors are removed from the dataset.  \r\n  \r\n**Check for highly correlated factors**\r\n\r\n```r\r\n# Remove user_name and classe columns\r\nfilteredDescr <- training[,2:53]\r\ndescrCor <-  cor(filteredDescr)\r\nhighlyCorDescr <- findCorrelation(descrCor, cutoff = .95)\r\nfilteredDescr <- filteredDescr[,-highlyCorDescr]\r\ndim(filteredDescr)\r\n```\r\n\r\n```\r\n## [1] 13737    47\r\n```\r\n\r\n```r\r\n# Add back user_name and classe columns\r\ntrainDescr <- cbind(filteredDescr, training[,c(1,54)])\r\ndim(trainDescr)\r\n```\r\n\r\n```\r\n## [1] 13737    49\r\n```\r\nTwo columns were highly correlated and these are removed.  \r\nSimilarly, the training data is also cleaned up.\r\n\r\n```r\r\nfilteredDescr <- testing[,2:53]\r\nfilteredDescr <- filteredDescr[,-highlyCorDescr]\r\ntestDescr <- cbind(filteredDescr, testing[,c(1,54)])\r\ndim(testDescr)\r\n```\r\n\r\n```\r\n## [1] 5885   49\r\n```\r\n\r\n### Train RandomForest\r\n\r\n```r\r\n# RandomForest\r\nstartTime <- Sys.time()\r\nmodFit <- randomForest(classe~., data=trainDescr)\r\nendTime <- Sys.time()\r\nendTime-startTime\r\n```\r\n\r\n```\r\n## Time difference of 26.75119 secs\r\n```\r\n\r\n#### Display model\r\n\r\n```r\r\nmodFit\r\n```\r\n\r\n```\r\n## \r\n## Call:\r\n##  randomForest(formula = classe ~ ., data = trainDescr) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 6\r\n## \r\n##         OOB estimate of  error rate: 0.5%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 3905    0    0    0    1 0.0002560164\r\n## B   14 2640    4    0    0 0.0067720090\r\n## C    0   14 2378    4    0 0.0075125209\r\n## D    0    0   25 2226    1 0.0115452931\r\n## E    0    0    2    4 2519 0.0023762376\r\n```\r\nFrom the model, we see that **Out of Bag** error was only 0.5%  \r\nIf there was no Overfitting, we should get a similar error on testing data  \r\n\r\n```r\r\nvarImpPlot(modFit, main=\"Model\")\r\n```\r\n\r\n![plot of chunk unnamed-chunk-13](figure/unnamed-chunk-13-1.png) \r\n\r\n\r\n```r\r\nplot(modFit, log=\"y\", main=\"Model\")\r\n```\r\n\r\n![plot of chunk unnamed-chunk-14](figure/unnamed-chunk-14-1.png) \r\n\r\n### Apply model on test data\r\n\r\n```r\r\naccuracy <- mean(predict(modFit, newdata=testDescr) == testDescr$classe)\r\ntestError <- (1-accuracy)*100\r\ntestError\r\n```\r\n\r\n```\r\n## [1] 0.7816483\r\n```\r\nThere is only **0.8%** error on the test data.  \r\nSo, there was no overfitting with the model.\r\n\r\n### Apply model to actual testing data to predict outcome\r\n\r\n```r\r\nfilteredDescr <- testData[,2:53]\r\nfilteredDescr <- filteredDescr[,-highlyCorDescr]\r\ntestDescr <- cbind(filteredDescr, testData[,c(1,54)])\r\ndim(testDescr)\r\n```\r\n\r\n```\r\n## [1] 20 49\r\n```\r\n\r\n```r\r\n# Final prediction\r\noutcome <- predict(modFit, newdata=testDescr)\r\noutcome\r\n```\r\n\r\n```\r\n##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \r\n##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B \r\n## Levels: A B C D E\r\n```\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}